<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ci on steinn.org</title>
    <link>http://localhost:1313/tags/ci/</link>
    <description>Recent content in Ci on steinn.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Feb 2016 20:57:15 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ci/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Continuous delivered Dockers via Codeship &#43; Elastic Beanstalk</title>
      <link>http://localhost:1313/post/docker-cd-codeship/</link>
      <pubDate>Wed, 24 Feb 2016 20:57:15 +0000</pubDate>
      
      <guid>http://localhost:1313/post/docker-cd-codeship/</guid>
      <description>

&lt;p&gt;At Takumi we&amp;rsquo;ve been using AWS Elastic Beanstalk for our different app
environments, for a variety of reasons.  Most importantly EB solves a lot
of problems straight out of the box, with minimal effort and a low learning
curve for most developers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Autoscale groups + ELB for scale and redundancy&lt;/li&gt;
&lt;li&gt;Deploy git commits and branches with easy CLI commands&lt;/li&gt;
&lt;li&gt;Easy to recreate and redeploy variations of environments&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However we&amp;rsquo;ve run into our fair share of niggling little issues, some of
which we&amp;rsquo;ve already solved, some which we are in the process of solving and
finally some which we won&amp;rsquo;t bother trying to solve.  The world is changing
fast and I believe we&amp;rsquo;ll be running on top of some container scheduler,
maybe ECS, but more likely Kubernetes, before the end of the year.&lt;/p&gt;

&lt;p&gt;In this blog post I&amp;rsquo;m going to describe how and why we decided to start
deploying pre-built dockers, which we build with Codeship and push to our
ECR repositories, and deploy using a tool built around generating &lt;code&gt;Dockerrun.aws.json&lt;/code&gt;
files based on extremely simple templates.&lt;/p&gt;

&lt;h1 id=&#34;why:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Why ?&lt;/h1&gt;

&lt;p&gt;Why deploy dockers, and why pre-build them?  In order to explain that I
need to explain how the historical Docker support in Elastic Beanstalk has
functioned.  We&amp;rsquo;ve been using it for a while for our web app, and it
basically works like this:&lt;/p&gt;

&lt;p&gt;If you set your EB environment type to one of the Docker types, and a
&lt;code&gt;Dockerfile&lt;/code&gt; is found in your project root, then upon deployment EB will
run something akin to:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;docker build -t aws_beanstalk/staging-app .
&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;docker run -d aws_beanstalk/staging-app
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;In addition to this, EB will inspect the newly started docker and generate
an upstream configuration file for nginx, pointing to the local docker and
the port defined by it&amp;rsquo;s &lt;code&gt;EXPOSE&lt;/code&gt; line:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;cat elasticbeanstalk-nginx-docker-upstream.conf
upstream docker &lt;span style=&#34;color: #f92672&#34;&gt;{&lt;/span&gt;
	server 172.17.0.5:5000&lt;span style=&#34;color: #f8f8f2&#34;&gt;;&lt;/span&gt;
	keepalive 256&lt;span style=&#34;color: #f8f8f2&#34;&gt;;&lt;/span&gt;
&lt;span style=&#34;color: #f92672&#34;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;This of course negates one of the major benefits of using dockers: building
once, and running anywhere.  If you have more than one app server, each one
will perform the build steps by themselves, not only wasting resources by
doing identical work, but also introducing the possibility for temporal
build issues affecting some nodes but not others, resulting in an inconsistent
build running on &amp;ldquo;identical&amp;rdquo; nodes.&lt;/p&gt;

&lt;p&gt;In my opinion one of the main reasons to use Docker in production is to
build once, run tests on the built artifact, and once deemed safe for deployment
that same tested &amp;ldquo;binary&amp;rdquo; is rolled out to the different app servers.  If your
backend is written in C++, Go or Java, you might find this pointless as you&amp;rsquo;re
used to being able to statically link a large binary or release a fully self-
contained jar.  However in Python, Ruby, and NodeJS things aren&amp;rsquo;t quite so
peachy.  Builds are slow, rely on remote and possibly fragile package repositories,
which always means builds are slow, and sometimes inconsistent!&lt;/p&gt;

&lt;p&gt;In our case this pattern of building on each node meant that we were basically
overprovisioned rather dramatically to speed up deployments and replacement
nodes.  Elastic Beanstalk takes care of utilizing local package caches so
that subsequent builds get faster as long as there are no major changes in
environment or dependencies.  Even so we faced harrowing 30+ minute build times
when spinning up new nodes, and deploying reasonable changes could take several
minutes per machine, and did I mention we were overprovisioned?  Between
deployments our machines would be 90-99% idle, no joke!&lt;/p&gt;

&lt;h1 id=&#34;how:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;How ?&lt;/h1&gt;

&lt;p&gt;So in order to build once and deploy that same tested artifact, what do you
need?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure your CI system to build a docker image and run your tests inside
the docker, and if you have external integration test scripts to run them
against a running container instance of your image.&lt;/li&gt;
&lt;li&gt;A docker registry or repository to store tagged versions of your images.
We decided to use ECR (Elastic Container Registry) because since January
16th they are natively supported by Elastic Beanstalk.  We tag all of our
builds with the githash (accessible as &lt;code&gt;$CI_COMMIT_ID&lt;/code&gt; from within codeship
containers.&lt;/li&gt;
&lt;li&gt;Only push and tag container images which pass tests, appropriately for
deployment or use further in your pipeline. We chose not to push any failing
containers to ECR.  That&amp;rsquo;s easy using Codeship&amp;rsquo;s &lt;code&gt;codeship-steps.yml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally you need a tool which allows you to reliably tell a particular
app environment that it should now run the image that you want.  We custom
built a little tool which uses &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;awsebcli&lt;/code&gt; and &lt;code&gt;boto3&lt;/code&gt; to deploy a
container with a specific tag, to a specific environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you want continuous delivery as well, as we do with our dev and staging
environments (we still pull the trigger manually for production, call me old
fashioned!) you need a way to allow Codeship, or a similar CI system to deploy
your code.  Since we had already setup our encrypted credentials with codeship
to allow them to pull and push from our AWS ECR repositories, we decided to
build a docker image with the tool from section &lt;em&gt;4&lt;/em&gt; here above, and push that
to a &lt;code&gt;utilities&lt;/code&gt; ECR repository which we could then use from Codeship.  Since
we re-used an IAM identity we setup for Codeship to deploy to Elastic Beanstalk
the old fashioned way, when we integrated with the ECR, re-using the same
credentials for our &amp;ldquo;deployment docker&amp;rdquo; was a walk in the park.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>