<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on steinn.org</title>
    <link>http://steinn.org/post/</link>
    <description>Recent content in Posts on steinn.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Mar 2016 09:57:15 +0000</lastBuildDate>
    <atom:link href="http://steinn.org/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Continuous delivered Dockers via Codeship &#43; Elastic Beanstalk</title>
      <link>http://steinn.org/post/docker-cd-codeship/</link>
      <pubDate>Wed, 23 Mar 2016 09:57:15 +0000</pubDate>
      
      <guid>http://steinn.org/post/docker-cd-codeship/</guid>
      <description>

&lt;p&gt;At &lt;a href=&#34;http://takumi.com/&#34;&gt;Takumi&lt;/a&gt; we recently started deploying immutable pre-built docker containers
through our CI system, onto our AWS Elastic Beanstalk nodes, while keeping
the deployment procedure as simple as:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;takumi-server (feature-branch) $ git commit -m &amp;quot;Important feature&amp;quot;
[feature-branch cf7edb2] Important feature
takumi-server (feature-branch) $ git push origin feature-branch
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;&amp;hellip; wait for codeship build&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;takumi-server (feature-branch) $ cd ../takumi-deployment
takumi-deployment (master) $ dpl deploy takumi-server git_cf7edb2 takumi-server-dev
Deploying &amp;#39;takumi-server:git_cf7edb2&amp;#39; to &amp;#39;takumi-server-dev&amp;#39;
...
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve been using AWS Elastic Beanstalk for our different app environments
for a variety of reasons.  Most importantly EB solves a lot of problems
straight out of the box, with minimal effort and a low learning curve for
most developers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Autoscale groups + ELB for scale and redundancy&lt;/li&gt;
&lt;li&gt;Deploy git commits and branches with easy CLI commands&lt;/li&gt;
&lt;li&gt;Easy to recreate and redeploy variations of environments&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However we&amp;rsquo;ve run into our fair share of niggling little issues, some of
which we&amp;rsquo;ve already solved, some which we are in the process of solving and
finally some which we won&amp;rsquo;t bother trying to solve.  The world is changing
fast and I believe we&amp;rsquo;ll be running on top of some container scheduler,
maybe ECS, but more likely Kubernetes, before the end of the year.&lt;/p&gt;

&lt;p&gt;In this blog post I&amp;rsquo;m going to describe how and why we decided to start
deploying pre-built dockers, which we build with Codeship and push to our
ECR repositories, and deploy using a tool built around generating &lt;code&gt;Dockerrun.aws.json&lt;/code&gt;
files based on extremely simple templates.&lt;/p&gt;

&lt;h1 id=&#34;why:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Why ?&lt;/h1&gt;

&lt;p&gt;Why deploy dockers, and why pre-build them?  In short, building and deploying
docker containers allows you to quickly and reliably build an image containing
all system dependencies and static configuration which can be tested and then
deployed via a registry to multiple app servers.&lt;/p&gt;

&lt;p&gt;Sounds pretty great, right? But I need to explain how the historical Docker
support in Elastic Beanstalk has functioned.  We&amp;rsquo;ve been using it for a while
for our web app, and it basically works like this:&lt;/p&gt;

&lt;p&gt;If you set your EB environment type to one of the Docker types, and a
&lt;code&gt;Dockerfile&lt;/code&gt; is found in your project root, then upon deployment EB will
run something akin to:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;$ docker build -t aws_beanstalk/staging-app .
$ docker run -d aws_beanstalk/staging-app
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;In addition to this, EB will inspect the newly started docker and generate
an upstream configuration file for nginx, pointing to the local docker and
the port defined by it&amp;rsquo;s &lt;code&gt;EXPOSE&lt;/code&gt; line:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;$ cat elasticbeanstalk-nginx-docker-upstream.conf
upstream docker {
	server 172.17.0.5:5000;
	keepalive 256;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;This of course negates one of the major benefits of using dockers: building
once, and running anywhere.  If you have more than one app server, each one
will perform the build steps by themselves, not only wasting resources by
doing identical work, but also introducing the possibility for temporal
build issues affecting some nodes but not others, resulting in an inconsistent
build running on &amp;ldquo;identical&amp;rdquo; nodes.&lt;/p&gt;

&lt;p&gt;In my opinion one of the main reasons to use Docker in production is to
build once, run tests on the built artifact, and once deemed safe for deployment
that same tested &amp;ldquo;binary&amp;rdquo; is rolled out to the different app servers.  If your
backend is written in C++, Go or Java, you might find this pointless as you&amp;rsquo;re
used to being able to statically link a large binary or release a fully self-
contained jar.  However in Python, Ruby, and NodeJS things aren&amp;rsquo;t quite so
peachy.  Builds are slow, rely on remote and possibly fragile package repositories,
which always means builds are slow, and sometimes inconsistent!&lt;/p&gt;

&lt;p&gt;In our case this pattern of building on each node meant that we were basically
overprovisioned rather dramatically to speed up deployments and replacement
nodes.  Elastic Beanstalk takes care of utilizing local package caches so
that subsequent builds get faster as long as there are no major changes in
environment or dependencies.  Even so we faced harrowing 30+ minute build times
when spinning up new nodes, and deploying reasonable changes could take several
minutes per machine, and did I mention we were overprovisioned?  Between
deployments our machines would be 90-99% idle, no joke!&lt;/p&gt;

&lt;h1 id=&#34;how:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;How ?&lt;/h1&gt;

&lt;p&gt;So in order to build once and deploy that same tested artifact, what do you
need?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure your CI system to build a docker image and run your tests inside
the docker, and if you have external integration test scripts to run them
against a running container instance of your image.&lt;/li&gt;
&lt;li&gt;A docker registry or repository to store tagged versions of your images.
We decided to use ECR (Elastic Container Registry) because since January
16th they are natively supported by Elastic Beanstalk.  We tag all of our
builds with the githash (accessible as &lt;code&gt;$CI_COMMIT_ID&lt;/code&gt; from within codeship
containers).&lt;/li&gt;
&lt;li&gt;Only push and tag container images which pass tests, appropriately for
deployment or use further in your pipeline. We chose not to push any failing
containers to ECR.  That&amp;rsquo;s easy using Codeship&amp;rsquo;s &lt;code&gt;codeship-steps.yml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally you need a tool which allows you to reliably tell a particular
app environment that it should now run the image that you want.  We custom
built a little tool which uses &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;awsebcli&lt;/code&gt; and &lt;code&gt;boto3&lt;/code&gt; to deploy a
container with a specific tag, to a specific environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;figure&gt;
    &lt;h4&gt;Rough diagram of our setup&lt;/h4&gt;
    &lt;img src=&#34;http://static.steinn.org/blog/post/codeship-docker-ci.png&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h5&gt;&lt;/h5&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The tool we built for step 4 (represented by a big red button in the image
above) is relatively trivial to make, the first working version took a few
hours and within a couple of days we had polished off the rough edges of our
first cut.  It basically performs the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Verifies that a docker with the given tag exist in ECR&lt;/li&gt;
&lt;li&gt;Verifies that the EB app and environment exist&lt;/li&gt;
&lt;li&gt;Generates a &lt;code&gt;Dockerrun.aws.json&lt;/code&gt; specifying the given docker tag&lt;/li&gt;
&lt;li&gt;Stages it for commit&lt;/li&gt;
&lt;li&gt;Deploys the current project to the specified EB app and environment using &lt;code&gt;--staged&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;(venv) $ dpl deploy takumi-server git_$GITHASH takumi-server-dev
Deploying &amp;#39;takumi-server:git_$GITHASH&amp;#39; to &amp;#39;takumi-srv-dev&amp;#39;
...
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;h1 id=&#34;codeship-settings:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Codeship Settings&lt;/h1&gt;

&lt;p&gt;If you have a Docker CI project setup with Codeship, you need to define two
files: &lt;code&gt;codeship-services.yml&lt;/code&gt; and &lt;code&gt;codeship-steps.yml&lt;/code&gt;.  The first one
defines a set of services, at least one of which should be based on your
application.  Other services we define are a postgres service based on
a standard postgres image (our integration test suite requires postgres),
a &lt;code&gt;dockercfg&lt;/code&gt; service which we use to authenticate against our docker
registry (ECR), and finally a deployment docker which we built around our
tool described in item &lt;em&gt;4&lt;/em&gt; in the &lt;em&gt;How&lt;/em&gt; section here above.&lt;/p&gt;

&lt;h2 id=&#34;codeship-steps-yml:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;codeship-steps.yml&lt;/h2&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #ae81ff&#34;&gt;app&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;build&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
    &lt;span style=&#34;color: #ae81ff&#34;&gt;image&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;&amp;lt;our-aws-account-id&amp;gt;.dkr.ecr.us-east-1.amazonaws.com/takumi-server&lt;/span&gt;
    &lt;span style=&#34;color: #ae81ff&#34;&gt;dockerfile_path&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;./Dockerfile&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;dockercfg_service&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;aws_generator&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;links&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
    &lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;postgres&lt;/span&gt;
&lt;span style=&#34;color: #ae81ff&#34;&gt;aws_generator&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;image&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;codeship/aws-ecr-dockercfg-generator&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;encrypted_env_file&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;aws_creds.encrypted&lt;/span&gt; &lt;span style=&#34;color: #75715e&#34;&gt;# contains Secret, AccessKey and Region&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;add_docker&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;true&lt;/span&gt;
&lt;span style=&#34;color: #ae81ff&#34;&gt;postgres&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;image&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;postgres:9.4&lt;/span&gt;
&lt;span style=&#34;color: #ae81ff&#34;&gt;deployment&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;image&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;&amp;lt;our-aws-account-id&amp;gt;.dkr.ecr.us-east-1.amazonaws.com/utilities:deployment&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;encrypted_env_file&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;aws_creds.encrypted&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;dockercfg_service&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;aws_generator&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Those of you familiar with &lt;code&gt;docker-compose&lt;/code&gt; might be familiar with the directives
here, because &lt;a href=&#34;https://codeship.com/documentation/docker/services/&#34;&gt;they
are related&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now to use these services you need to define your build steps:&lt;/p&gt;

&lt;h2 id=&#34;codeship-steps-yml-1:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;codeship-steps.yml&lt;/h2&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;serial&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;continuous delivery&lt;/span&gt;
  &lt;span style=&#34;color: #ae81ff&#34;&gt;steps&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
    &lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;service&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;app&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;command&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;make docker_test&lt;/span&gt;
    &lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;push docker&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;steps&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt;
      &lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;with githash&lt;/span&gt;
        &lt;span style=&#34;color: #ae81ff&#34;&gt;type&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;push&lt;/span&gt;
        &lt;span style=&#34;color: #ae81ff&#34;&gt;service&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;app&lt;/span&gt;
        &lt;span style=&#34;color: #ae81ff&#34;&gt;registry&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;https://&amp;lt;our-aws-account-id&amp;gt;.dkr.ecr.us-east-1.amazonaws.com&lt;/span&gt;
        &lt;span style=&#34;color: #ae81ff&#34;&gt;image_name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;&amp;lt;our-aws-account-id&amp;gt;.dkr.ecr.us-east-1.amazonaws.com/takumi-server&lt;/span&gt;
        &lt;span style=&#34;color: #ae81ff&#34;&gt;image_tag&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;git_{{ .CommitID }}&lt;/span&gt;
    &lt;span style=&#34;color: #f8f8f2&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;name&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;deployment&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;service&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;deployment&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;command&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;takumi-server git_{{ .CommitID }} takumi-srv-dev&lt;/span&gt;
      &lt;span style=&#34;color: #ae81ff&#34;&gt;tag&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;master&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;In our steps file we have three steps: &lt;em&gt;test&lt;/em&gt;, which starts a docker based on
the &lt;code&gt;app&lt;/code&gt; service with the command &lt;code&gt;make docker_test&lt;/code&gt;.  Our Makefile has a
target which runs the tests with the appropriate configuration for a docker
which is linked to another docker named &lt;code&gt;postgres&lt;/code&gt;.  Our second step is called
&lt;em&gt;push docker&lt;/em&gt; and it specifies how to tag and push the docker built by our
&lt;code&gt;app&lt;/code&gt; service.  Finally we have a &lt;em&gt;deployment&lt;/em&gt; step which runs an instance of
our &lt;code&gt;deployment&lt;/code&gt; service, with the command required to trigger a deployment.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;
    &lt;h4&gt;Pull request merged to master and deployed to dev&lt;/h4&gt;
    &lt;img src=&#34;http://static.steinn.org/blog/post/codeship-cd-build-success.png&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h5&gt;&lt;/h5&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;developing-steps:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Developing Steps&lt;/h2&gt;

&lt;p&gt;When developing these steps I found that Codeship&amp;rsquo;s &lt;a href=&#34;https://codeship.com/documentation/docker/installation/&#34;&gt;
Jet&lt;/a&gt; tool was very helpful.  Otherwise be prepared for a lot of commits and
pushes to GitHub to trigger builds which fail after a few minutes.  Installing
Jet and being able to execute the steps locally and figure out any errors in
our step or service definitions within a seconds was a huge help:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;$ jet steps
...
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;deployment-step:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Deployment Step&lt;/h2&gt;

&lt;p&gt;The deployment step specifies &lt;code&gt;tag: master&lt;/code&gt; which means it should only execute
for our &lt;code&gt;master&lt;/code&gt; branch.  Personally I wish Codeship would have named this
directive &lt;code&gt;branch&lt;/code&gt; instead of &lt;code&gt;tag&lt;/code&gt;.  That would have been more intuitive for
our use case, since dockers already have their own tags.  The directive however
works for git tags as well, so that explains the name.&lt;/p&gt;

&lt;p&gt;Since the steps are &lt;code&gt;serial&lt;/code&gt;, as specified at the top of the document, each
step only executes if the previous step was succesful.  For our deployments
we built a docker around the deployment tool described above, with a helper
script as an entrypoint which prepends &lt;code&gt;dpl deploy&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&#34;further-reading:ef09fe0db0b31cdf5742f88c311eb1fe&#34;&gt;Further Reading&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re interested in a more complete overview of the technologies mentioned
here, here are some helpful links:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://codeship.com/documentation/docker/services&#34;&gt;Codeship Services &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://codeship.com/documentation/docker/steps&#34;&gt;Codeship Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html&#34;&gt;Deploying Elastic Beanstalk Applications from Docker Containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.codeship.com/aws-registry/&#34;&gt;Integrating Your Codeship CI/CD Pipeline with AWS ECR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.codeship.com/introducing-jet-codeships-platform-for-docker/&#34;&gt;Introducing Jet, Codeship’s Platform for Docker, and the Future of CI&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Flask and StatsD revisited</title>
      <link>http://steinn.org/post/flask-statsd-revisited/</link>
      <pubDate>Tue, 15 Mar 2016 21:28:49 +0000</pubDate>
      
      <guid>http://steinn.org/post/flask-statsd-revisited/</guid>
      <description>&lt;p&gt;A couple of months ago I wrote about &lt;a href=&#34;http://steinn.org/post/flask-statsd/&#34;&gt;automatically emitting&lt;/a&gt;
statsd metrics for Flask views using a &lt;a hre_f=&#34;http://werkzeug.pocoo.org/&#34;&gt;Werkzeug&lt;/a&gt;
middleware.&lt;/p&gt;

&lt;p&gt;There was one rather large caveat though: how I dealt with dynamic URL
parameters.  Optimally I would have matched the incoming request against
the available URL rules or routes in Flask, and used the URL rule metadata
to generate a stable metric name.  To save time I relied upon the fact that
all dynamic URL parts in our API were at the time UUID&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Since then I&amp;rsquo;ve found the time to update it and use the URL map. So here&amp;rsquo;s
the updated gist:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/32aad08050f642245dd1.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also updated the original &lt;a href=&#34;http://steinn.org/post/flask-statsd/&#34;&gt;Flask and StatsD&lt;/a&gt;
post itself, and the &lt;a href=&#34;https://github.com/steinnes/statsdmiddleware&#34;&gt;example
github repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On a humorous note, let me tell you it definitely took me a few seconds to
realize why I was suddenly seeing &lt;code&gt;phpmyadmin.&amp;lt;...&amp;gt;&lt;/code&gt; metrics in the &lt;a href=&#34;https://www.datadoghq.com&#34;&gt;Datadog&lt;/a&gt;
metric explorer.  Then I remembered the technical debt incurred by my
initial approach.  It&amp;rsquo;s now been a little while since we fixed it, and it
sure feels good to share the improved solution :-)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adding swap to Elastic Beanstalk</title>
      <link>http://steinn.org/post/elasticbeanstalk-swap/</link>
      <pubDate>Mon, 14 Mar 2016 09:02:25 +0000</pubDate>
      
      <guid>http://steinn.org/post/elasticbeanstalk-swap/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re using small t2 instances on Elastic Beanstalk which only have 1GB or
less of memory, you may run out of memory while doing rare, yet memory intensive
tasks such as deploying (and/or building) your app.&lt;/p&gt;

&lt;p&gt;If your app does not require a lot of memory except during those rare moments,
increasing your instance size and being overprovisioned 99% of the time is not a
solution which should satisfy a good engineer.&lt;/p&gt;

&lt;p&gt;A better solution would be to add swap space to the machine, which on Linux is
something you can easily do at runtime, even though most distributions will
guide you towards setting up a separate swap partition.  A swapfile can be
created with these three simple commands:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #75715e&#34;&gt;# dd if=/dev/zero of=/var/swapfile bs=1M count=1024   # create a 1GB file of 0&amp;#39;s&lt;/span&gt;
&lt;span style=&#34;color: #75715e&#34;&gt;# mkswap /var/swapfile&lt;/span&gt;
&lt;span style=&#34;color: #75715e&#34;&gt;# swapon /var/swapfile&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;The simplest way I know to do this with Elastic Beanstalk is to use &lt;a href=&#34;http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html&#34;&gt;
.ebextensions&lt;/a&gt;.  Simply create a folder in your elastic beanstalk deployable
project named &lt;code&gt;.ebextensions&lt;/code&gt;, and within it add two files, the first a more
elaborate version of the three commands above, something akin to this script,
and call it &lt;code&gt;setup_swap.sh&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/3a1de7250de10687ade1.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Then add a &lt;code&gt;.config&lt;/code&gt; file which tells EB to run the setup script on deployment.
The script checks if a swapfile already exists, and if so exits peacefully.&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/1f8a1b44fed4b136005f.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Once done your project directory tree should look partially like this:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;project
├── .ebextensions
│   ├── 01_setup_swap.config
│   └── setup_swap.sh
...
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;If you inspect your &lt;code&gt;/var/log/eb-activity.log&lt;/code&gt; you should see something like this:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:43:31.452Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Starting activity...
&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:43:31.722Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild/Infra-EmbeddedPostBuild&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Starting activity...
&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:43:31.723Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild/Infra-EmbeddedPostBuild/postbuild_0_project&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Starting activity...
&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:43:31.943Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild/Infra-EmbeddedPostBuild/postbuild_0_project/Command 01setup_swap&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Starting activity...
&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:44:24.967Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild/Infra-EmbeddedPostBuild/postbuild_0_project/Command 01setup_swap&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Completed activity. Result:
  2048+0 records in
  2048+0 records out
  &lt;span style=&#34;color: #ae81ff&#34;&gt;2147483648&lt;/span&gt; bytes &lt;span style=&#34;color: #f92672&#34;&gt;(&lt;/span&gt;2.1 GB&lt;span style=&#34;color: #f92672&#34;&gt;)&lt;/span&gt; copied, 51.3331 s, 41.8 MB/s
  Setting up swapspace version 1, &lt;span style=&#34;color: #f8f8f2&#34;&gt;size&lt;/span&gt; &lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color: #ae81ff&#34;&gt;2097148&lt;/span&gt; KiB
  no label, &lt;span style=&#34;color: #f8f8f2&#34;&gt;UUID&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;b4c8a86f-e34d-43f2-8bc0-21c97aa5223e

&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;2016-03-15T03:44:24.967Z&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; INFO  &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;8044&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;  - &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;Application update/AppDeployStage0/EbExtensionPostBuild/Infra-EmbeddedPostBuild/postbuild_0_project&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt; : Completed activity.
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;And finally you should see your new swap space:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;ec2-user@ip-172-31-40-245 var&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;free
             total       used       free     shared    buffers     cached
Mem:       &lt;span style=&#34;color: #ae81ff&#34;&gt;1019452&lt;/span&gt;    &lt;span style=&#34;color: #ae81ff&#34;&gt;1005548&lt;/span&gt;      &lt;span style=&#34;color: #ae81ff&#34;&gt;13904&lt;/span&gt;       &lt;span style=&#34;color: #ae81ff&#34;&gt;2308&lt;/span&gt;        &lt;span style=&#34;color: #ae81ff&#34;&gt;248&lt;/span&gt;      43104
-/+ buffers/cache:     &lt;span style=&#34;color: #ae81ff&#34;&gt;962196&lt;/span&gt;      57256
Swap:      &lt;span style=&#34;color: #ae81ff&#34;&gt;2097148&lt;/span&gt;      &lt;span style=&#34;color: #ae81ff&#34;&gt;18868&lt;/span&gt;    2078280
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ebenv: Elastic Beanstalk environment copy/dump tool</title>
      <link>http://steinn.org/post/ebenv-tool/</link>
      <pubDate>Wed, 24 Feb 2016 23:30:28 +0000</pubDate>
      
      <guid>http://steinn.org/post/ebenv-tool/</guid>
      <description>&lt;p&gt;At &lt;a href=&#34;http://takumi.com&#34;&gt;Takumi&lt;/a&gt; we use Elastic Beanstalk, which
is a nice API/service from AWS which strikes a decent balance between a
heroku-esque simplified service interface for developers, and the hands-on
control required by those of the devops persuasion, such as myself.&lt;/p&gt;

&lt;p&gt;One of the main benefits of using Elastic Beanstalk is how quick and easy
it is to spin up new environments, and I make heavy use of that in my work.
Basically what I find relatively often useful is the ability to copy an
existing environment to test new code with the same configuration and
resources, with a limited set of clients.  Reasons to do that might be;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rolling out new features to a limited set of clients&lt;/li&gt;
&lt;li&gt;Making a new environment for a new developer who wants to test something
with minimal risk of disruption&lt;/li&gt;
&lt;li&gt;Making new development environments with &amp;ldquo;production-esque&amp;rdquo; configuration
as needed, and as the app needs evolve&lt;/li&gt;
&lt;li&gt;Keeping a separate environment which gets 5% of traffic via weighted DNS
(canary-ing)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basically it&amp;rsquo;s a logical conclusion that once you treat your infrastructure
as programmable outputs which can be recreated and discarded at will, that
your development patterns and needs evolve to take advantage of that.&lt;/p&gt;

&lt;p&gt;One thing though that slightly bugged me when dealing with Elastic Beanstalk
was the fact that there was no programmatic way of doing the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Dump the environment variables for an app env to an envfile, usable by
&lt;a href=&#34;http://ddollar.github.io/foreman/&#34;&gt;foreman&lt;/a&gt;, &lt;a
href=&#34;https://github.com/nickstenning/honcho&#34;&gt;honcho&lt;/a&gt;
or similar tools&lt;/li&gt;
&lt;li&gt;Same as &lt;em&gt;1&lt;/em&gt; but an envdir, usable by &lt;a
href=&#34;https://cr.yp.to/daemontools/envdir.html&#34;&gt;envdir&lt;/a&gt;, &lt;a
href=&#34;http://smarden.org/runit/chpst.8.html&#34;&gt;chpst&lt;/a&gt; or similar tools&lt;/li&gt;
&lt;li&gt;Copy the environment variables (config) from one environment to another&lt;/li&gt;
&lt;li&gt;Load the environment variables in an envfile or an envdir into an
existing EB environment&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Recently I was doing a lot of work and needed to replace some of our existing
environments because we&amp;rsquo;re migrating everything from language specific EB
environments to the Docker environments, so I basically needed to create new
EB environments for existing ones, but I couldn&amp;rsquo;t just clone them because
the platform gets cloned as well and you can&amp;rsquo;t change the platform, only the
version:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;
    &lt;h4&gt;&amp;#34;changing&amp;#34; a platform version :-)&lt;/h4&gt;
    &lt;img src=&#34;http://static.steinn.org/blog/post/change-platform.png&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h5&gt;&lt;/h5&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Doing this I sorely missed the ability to copy the environment configuration
because I was essentially making clones of existing environments without
being able to actually &lt;em&gt;clone&lt;/em&gt; the environments.&lt;/p&gt;

&lt;p&gt;Another semi-common issue I encountered was dumping values from an existing
environment, in order to start a server locally or on another instance using
the same configuration.  Basically, over the last few months I&amp;rsquo;d been
intermittently wishing for a tool to deal with EB environment configs more
programmatically, partially because we have a decent amount of configuration
values which don&amp;rsquo;t play nice with &lt;a
href=&#34;https://github.com/gxela/awsebcli&#34;&gt;awsebcli&lt;/a&gt;, but also because of
the other issues I described already.&lt;/p&gt;

&lt;p&gt;I decided to finally do something about it and I wrote &lt;a
href=&#34;https://github.com/steinnes/ebenv&#34;&gt;ebenv&lt;/a&gt;.  You can install it via pip
(&lt;code&gt;pip install ebenv&lt;/code&gt;) or find it on &lt;a
href=&#34;https://github.com/steinnes/ebenv&#34;&gt;github.com/steinnes/ebenv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Currently it can handle issues 1-3 here above, and I&amp;rsquo;m planning to add
counterpart commands to the &lt;code&gt;env&lt;/code&gt; and &lt;code&gt;envdir&lt;/code&gt; commands to easily sync
local changes (which might even be maintained in a &lt;em&gt;gasp&lt;/em&gt; repository!)
with a remote Elastic Beanstalk environment.&lt;/p&gt;

&lt;p&gt;If you also use Elastic Beanstalk, I hope ebenv can be of some use to you,
I certainly longed for a tool of this nature for a while before finally
writing it ;-)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Flask and StatsD</title>
      <link>http://steinn.org/post/flask-statsd/</link>
      <pubDate>Mon, 30 Nov 2015 09:58:33 +0000</pubDate>
      
      <guid>http://steinn.org/post/flask-statsd/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve written about &lt;a href=&#34;http://steinn.org/post/reinventing-the-wheel&#34;&gt;StatsD&lt;/a&gt; &lt;a href=&#34;http://steinn.org/post/datadog&#34;&gt;before&lt;/a&gt;,
but it&amp;rsquo;s time to pick up the pen again, at least in the proverbial sense (I&amp;rsquo;m
typing this on a laptop-chiclet keyboard).&lt;/p&gt;

&lt;p&gt;A few months ago I joined a new company: &lt;a href=&#34;http://takumi.com&#34;&gt;Takumi&lt;/a&gt;
and we&amp;rsquo;re building an iOS app married to a Python Flask-based backend. From
many perspectives, a very similar tech stack as we used at QuizUp.  And it&amp;rsquo;s no
coincidence.  One of the founders of Takumi is &lt;a href=&#34;http://twitter.com/jokull&#34;&gt;Jökull Sólberg&lt;/a&gt;,
who I worked closely with at QuizUp and as far as I know had a major effect on
the initial technology stack used there.&lt;/p&gt;

&lt;p&gt;Having used this technology to build a product which got a million active users
in little over a week, I think it&amp;rsquo;s a fair assumption that we are comfortable
with the problems and solutions presented on the backend by this choice of tech.
But it&amp;rsquo;s always possible to tweak, and that&amp;rsquo;s what this blog post is about.&lt;/p&gt;

&lt;p&gt;At QuizUp, one of the lessons I learned, and I&amp;rsquo;ve openly &lt;a href=&#34;https://speakerdeck.com/steinnes/quizup-zero-to-a-million-users-in-8-days?slide=16&#34;&gt;spoken of&lt;/a&gt;
is the importance of metrics and instrumenting code, specifically using StatsD
and &lt;a href=&#34;http://www.datadog.com&#34;&gt;Datadog&lt;/a&gt; is my preferred service for
making those metrics visible and reacting to them.  We generally did this
using a combination of two techniques:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A fairly ingenious view decorator which took the metric name as a parameter,
wrapped a Flask view and and emitted a statsd timing for the view.  Instant
visibility into your view latencies!  I credit &lt;a href=&#34;https://twitter.com/johannth&#34;&gt;Jói&lt;/a&gt;,
the CTO of QuizUp for this.  He&amp;rsquo;s an amazing developer and generally one of the
nicest human beings you&amp;rsquo;ll ever meet.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For those views which did a lot of things, we wrote a little context manager
which could do the same as the view decorator, but for any code block.  This
piece of code was born as our &amp;ldquo;home&amp;rdquo; endpoint (a big-fat-gimme-everything for
the client home screen) started becoming slower and slower.  We needed some
way of monitoring which parts of the response were taking the longest to retrieve
or generate.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As I mentioned before, now I&amp;rsquo;m with another company and doing similar things, and
as I&amp;rsquo;m wont to do, I try and improve/iterate on what I&amp;rsquo;m doing.  At Takumi I
started implementing a similar view decorator, meaning some potentially repetitive
code like this everywhere:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/507d894df8f445172c8b.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Basically adding a &lt;code&gt;view_metric&lt;/code&gt; decorator around every view with a metric
name which should be easily derivable from the view name or request path.  First
I thought &amp;ldquo;Ah, the metric name should be derived from the request endpoint name!&amp;rdquo;,
but replacing &lt;code&gt;@view_metric(&amp;quot;...&amp;quot;)&lt;/code&gt; with &lt;code&gt;@view_metric&lt;/code&gt; for over a hundred
different views still is quite repetitive.&lt;/p&gt;

&lt;p&gt;Thinking back I remembered having used Werkzeug&amp;rsquo;s &lt;a href=&#34;http://werkzeug.pocoo.org/docs/0.10/contrib/profiler/&#34;&gt;ProfilingMiddleware&lt;/a&gt;
back when performance tuning QuizUp, so I thought I&amp;rsquo;d explore writing my own
StatsD MiddleWare for Flask.:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/32aad08050f642245dd1.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;&lt;s&gt;There&amp;rsquo;s one pretty big assumption here, which is that the all dynamic parts of
a URL are UUIDs.  Look at &lt;code&gt;def _metric_name_from_path&lt;/code&gt; for details, but there
I basically use a regular expression to replace any UUID&amp;rsquo;s in the URL string
with &lt;code&gt;id&lt;/code&gt;.  Probably a more robust approach would be to parse the request path
and construct it out of safe characters found between slashes.  I&amp;rsquo;ll see if
there&amp;rsquo;s a smarter way if I make a proper flask plugin out of this.&lt;/s&gt;
Update: I rewrote the metric name construction to use the Flask app&amp;rsquo;s URL map.
See &lt;a href=&#34;http://steinn.org/post/flask-statsd-revisited/&#34;&gt;this post&lt;/a&gt; for details :-)&lt;/p&gt;

&lt;p&gt;The code above includes a couple of little extras:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;StatsD&lt;/code&gt;: a statsd wrapper I wrote to automatically add certain tags to all
metrics emitted.  That&amp;rsquo;s something I&amp;rsquo;ve done before, and not really related to
Flask or Werkzeug, but we are using this at Takumi so I decided to include it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get_cpu_time&lt;/code&gt;: the middleware and the context manager both always retrieve
the actual cpu time and submit that as a separate metric.  This is a neat
little trick I suppose most devops people will be used to, and can be very
helpful to detect both wasted cpu cycles and external bottlenecks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here&amp;rsquo;s a minimal example of how to use the middleware:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/ebe4b170a46b3b74d7da.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;To time specific blocks of your code you can import &lt;code&gt;TimingStats&lt;/code&gt; and use it
directly:
&lt;script src=&#34;https://gist.github.com/2cd21b3fa2877cdf8ac4.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;We launched Takumi on the 11th of November and we&amp;rsquo;re using this code in
production, and although I have a nagging suspicion I&amp;rsquo;ll discover some unpaid
price for this magic, we are seeing metrics for all of our views completely
automatically :-)&lt;/p&gt;

&lt;p&gt;For convenience sake if anyone wants to use this stuff, here&amp;rsquo;s a github repo:
&lt;a href=&#34;https://github.com/steinnes/statsdmiddleware&#34;&gt;github.com/steinnes/statsdmiddleware&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up APNS for iOS</title>
      <link>http://steinn.org/post/apns-setup/</link>
      <pubDate>Thu, 13 Aug 2015 16:37:37 +0000</pubDate>
      
      <guid>http://steinn.org/post/apns-setup/</guid>
      <description>&lt;p&gt;Last week we started setting up push notifications on the iOS app we are building
at work.  I knew from my time at QuizUp that this is more complicated than
you&amp;rsquo;d think, but still I was surprised at the complexity Apple front-loads on
people developing for their platforms.&lt;/p&gt;

&lt;p&gt;I read a couple of guides which were either slightly too vague, or really
detailed but contained directions for outdated versions of one of the many
graphical user interfaces Apple seems to think developers enjoy (hint: most
of us do not!).&lt;/p&gt;

&lt;p&gt;First I attempted what I usually do: to understand what I am doing before
committing to any potentially confusing changes.  After spending some time
with this layercake of cryptographic overengineering I abandoned hope of
abstract understanding and decided to learn by doing.&lt;/p&gt;

&lt;p&gt;Thirty hours later I was left with a whole bunch of useless keys,
certificates, provisioning profiles, app ID&amp;rsquo;s and various unspeakable things
done to my local keychain and our iOS &amp;ldquo;Certificates, Identifiers &amp;amp; Profiles&amp;rdquo;
in the Apple Developer Center.  But I ended up with these steps here.&lt;/p&gt;

&lt;p&gt;I was helped &lt;em&gt;a lot&lt;/em&gt; by this &lt;a href=&#34;http://www.raywenderlich.com/32960/apple-push-notification-services-in-ios-6-tutorial-part-1&#34;&gt;excellent guide&lt;/a&gt;
by Ray Wenderlich.  I pretty much followed his steps, but use the &lt;code&gt;openssl&lt;/code&gt;
command line tool instead of Apple&amp;rsquo;s &amp;ldquo;Certificate Assistant&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;So without further ado, here are the steps I followed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In your terminal of choice, Generate private key
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;    openssl genrsa -out com.example.app.key 2048
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In that same terminal, create a CSR (Certificate Signing Request)
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;openssl req -new &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
  -key com.example.app.key &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
  -out com.example.app.csr &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
  -subj &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;/emailAddress=you@example.com/CN=com.example.app/C=IS/O=Example Ltd&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

I recommend keeping these two files somewhere safe, you&amp;rsquo;ll need them in step 5.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Log into the Apple Developer console. Go to &amp;ldquo;Certificates, Identifiers &amp;amp; Profiles&amp;rdquo;,
and create a new App ID, I&amp;rsquo;ll use &lt;code&gt;com.example.app&lt;/code&gt; for this guide.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Again in the Apple Developer console, Click &amp;ldquo;Edit&amp;rdquo; under &amp;ldquo;Application Services&amp;rdquo;, and scroll down to &amp;ldquo;Push Notifications&amp;rdquo;, click there &amp;ldquo;Create Certificate&amp;rdquo;.
Use the CSR created earlier (&lt;code&gt;com.example.app.csr&lt;/code&gt;) to create the certificate, download it, it will have a filename like &lt;code&gt;aps_development.cer&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Convert the .cer file into a PEM file:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;   openssl x509 -in aps_development.cer -inform der -out com.example.app.aps_developement.pem
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Test your key and certificate against APNS:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;openssl s_client -connect gateway.sandbox.push.apple.com:2195 -cert com.example.app.aps_developement.pem -key com.example.app.key
&lt;/pre&gt;&lt;/div&gt;

If everything is OK, you should see a &amp;ldquo;CONNECTED..&amp;rdquo; string, followed by a succesful SSL handshake.  You can stop by sending EOF (^D).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Combine the private key and the aps_development certificate into a single PEM file:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;cat com.example.app.key com.example.app.aps_developement.pem &amp;gt;&amp;gt; apns_com.example.app_combined.pem
&lt;/pre&gt;&lt;/div&gt;

You will use this file later when sending notifications.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go into the Apple Developer Console, open &amp;ldquo;Provisioning Profiles&amp;rdquo; click the &amp;ldquo;+&amp;rdquo; icon.  There choose &amp;ldquo;iOS Development&amp;rdquo;, then click continue.
There choose the App ID created earlier (com.example.app) and click continue.  Then choose the developer certificates and the devices, finally click &amp;ldquo;generate&amp;rdquo;.
Choose a name for the profile and then download it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once you&amp;rsquo;ve downloaded your profile (should have a .mobileprovision suffix) drag it into XCode, or open it in XCode somehow.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now what you do with the provisioning profile is something I&amp;rsquo;ll leave to my
colleague &lt;a href=&#34;http://blog.trauzti.com&#34;&gt;Trausti&lt;/a&gt; to explain, since
he wrote the client code.  Until then you can check out Ray&amp;rsquo;s guide I mentioned
here above, and figure out how to get your very own device token.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve gotten your device token for your app using the same bundle
identifier as you used in my steps above (&lt;code&gt;com.example.app&lt;/code&gt;) you&amp;rsquo;ll want to
test it.  Since I&amp;rsquo;m mostly using python these days, and Ray&amp;rsquo;s example was in
PHP (which works fine btw!), I decided to implement an APNS test script
in python: &lt;a href=&#34;http://github.com/steinnes/apnsend&#34;&gt;apnsend&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;git clone git@github.com:steinnes/apnsend.git
&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ cd &lt;/span&gt;apnsend
&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;make 
&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;venv/bin/apnsend apns_com.example.app_combined.pem token &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;hello from apnsend&amp;quot;&lt;/span&gt; -s
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;The script takes as a parameter the PEM file containing your key and the
certificate generated by apple, and a device token requested from an app
with the bundle id, which was built using the provisioning profile.&lt;/p&gt;

&lt;p&gt;The end result should look something like this:
&lt;figure&gt;
    &lt;h4&gt;First push notification&lt;/h4&gt;
    &lt;img src=&#34;http://static.steinn.org/blog/post/apnsend-test1.jpg&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h5&gt;&lt;/h5&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring at QuizUp: Datadog</title>
      <link>http://steinn.org/post/datadog/</link>
      <pubDate>Thu, 16 Apr 2015 21:08:49 +0000</pubDate>
      
      <guid>http://steinn.org/post/datadog/</guid>
      <description>&lt;p&gt;One of the services we use a lot for monitoring at QuizUp is Datadog.&lt;/p&gt;

&lt;p&gt;We use several software-as-a-service&amp;rsquo;s, but this one is one of my absolute
favorites.  Datadog is an aggregator for StatsD metrics, and StatSD is
as &lt;a href=&#34;http://steinn.org/post/reinventing-the-wheel/&#34;&gt;I have mentioned before&lt;/a&gt;, one
of my favorite technologies.  The way we use Datadog is both through their
large set of excellent &lt;a href=&#34;https://www.datadoghq.com/integrations/&#34;&gt;
integrations&lt;/a&gt;, which retrieve a host of standard metrics from every
third party service in our stack, but also monitor all of our hosts for
standard operating system metrics like load average, disk usage, memory
usage broken down by application, cpu load, etc; and we&amp;rsquo;ve written our
&lt;a href=&#34;https://gist.github.com/steinnes/04d016d19def45f85a18&#34;&gt;custom
integrations&lt;/a&gt; if one did not exist before.&lt;/p&gt;

&lt;p&gt;Additionally we instrument our applications with statsd calls which feed
important higher level metrics into datadog where all of these can be
combined in powerful dashboards and used by metric alerts to trigger
e-mails, pings to Slack or PagerDuty, etc.  These metric alerts are our
first line of defence, allowing us to detect and fix problems before they
affect users, so effectively they keep our uptime good.  Whether it&amp;rsquo;s
a sudden increase in the number of API non-2xx, increased latencies, disks
filling up, elevated service health transitions, cluster size changes,
no matter; it&amp;rsquo;s all fed into Datadog and we&amp;rsquo;re alerted from there.  This
is the kind of service, which if it proved flaky, I would get very
frustrated very quickly.  It&amp;rsquo;s never been flaky.  No pressure ;-)&lt;/p&gt;

&lt;p&gt;The purpose of this post is however to point out a fantastic recent
feature which I learned about this week, and might just change the way
we work when examining incidents.  Until now I&amp;rsquo;ve been heavily utilizing
the Screenshot+Dropbox+Copybuffer integration offered by Dropbox, to basically
screengrab a portion of a graph or a dashboard, then paste the link to the
relevant Slack channel and ping people who I believe should investigate.&lt;/p&gt;

&lt;p&gt;But there&amp;rsquo;s an easier way to do this, &lt;a href=&#34;https://www.datadoghq.com/2015/03/real-time-graph-annotations/&#34;&gt;
click the snapshot icon&lt;/a&gt; and tag the corresponding people, or use a hashtag
to add custom tags which can be used with event timeline searches.  Also if you
have Slack integrated, you can tag the relevant Slack channels directly from
the comment. This way the discussion of any issue can happen in context, and
the comments and thoughts of the engineers working on the issue will be seen
by anyone viewing that particular graph, at that time, later.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;
    &lt;h4&gt;Commenting on an anomaly&lt;/h4&gt;
    &lt;img src=&#34;http://static.steinn.org/blog/post/datadog-annotate.png&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h5&gt;&lt;/h5&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;This week I had the pleasure of meeting Alexis (&lt;a
href=&#34;http://twitter.com/alq&#34;&gt;@alq&lt;/a&gt;) from Datadog again, and he showed
me this feature amongst others, and also signed me up for notifications when
they start beta testing new features.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re running an app and would like to give Datadog a try, but feel the
per-host fee is expensive, it&amp;rsquo;s possible to setup a single datadog statsd
collection node and make your other nodes send their metrics to that node.
This would not get you their nice host-level monitoring, but it&amp;rsquo;s a cheap
way to try.  At QuizUp we monitor all our hosts though, as when running larger
instances the fee charged per host is trivial for us, so I highly recommend
that if cost is not an issue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>redis workload recorder</title>
      <link>http://steinn.org/post/redis-workload-recorder/</link>
      <pubDate>Thu, 19 Mar 2015 01:11:52 +0000</pubDate>
      
      <guid>http://steinn.org/post/redis-workload-recorder/</guid>
      <description>&lt;p&gt;Recently I&amp;rsquo;ve been working on some redis profiling and research, and since
we run several distinct redis instances at Plain Vanilla (due to reliability
and configuration reasons) the necessity to record a certain amount of commands
for several different redis instances became rather apparent.&lt;/p&gt;

&lt;p&gt;Rather than relying on my usual &lt;code&gt;redis-cli monitor &amp;gt; /some/file&lt;/code&gt; inside a
screen, then remembering to &lt;code&gt;ctrl+c&lt;/code&gt; after say 3600 seconds, I decided to
script this for the benefit of my future self (and maybe others).&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/ead90afa9882e89b995f.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Usage:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;screen
&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;./redis-workload-record.py &lt;span style=&#34;color: #ae81ff&#34;&gt;3600&lt;/span&gt; localhost &amp;gt; 3600-sec-redis-something.log
^A d
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;This will start a screen where the workload recorder will run for 3600 seconds.
As can be seen from the code, the exit message will be printed to stderr so
your output can be safely redirected into a file, which then can be read and
replayed by something like this:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/212f3273d17a7549ac46.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;By running something like this:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;cat 3600-sec-redis-something.log &lt;span style=&#34;color: #f8f8f2&#34;&gt;|&lt;/span&gt; python redis-pipe-commands.py
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Hope this benefits someone (other than my future self :-)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinventing the wheel</title>
      <link>http://steinn.org/post/reinventing-the-wheel/</link>
      <pubDate>Mon, 09 Mar 2015 16:03:03 +0000</pubDate>
      
      <guid>http://steinn.org/post/reinventing-the-wheel/</guid>
      <description>

&lt;p&gt;We all tend to reinvent the wheel once in a while, and often it&amp;rsquo;s just
a question of not having found the right tool with a quick search, or
better: we prefer doing something differently and/or we enjoy the process
of building something just for the heck of it.  This is a small story of
me being slightly stupid, but also about UDP and how awesome it is :-)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fond of UDP.  The user datagram protocol.  Unlike it&amp;rsquo;s cousin TCP,
UDP packets do not contain a sequence number and the protocol does not
ack packets.  For practical purposes this means it gets used in scenarios
where latency is more important than reliability: you prefer to lose some
packets, rather than adding any overhead of acknowledging packets or waiting
for packets if they arrive out-of-sequence or get lost on the way.&lt;/p&gt;

&lt;p&gt;When streaming audio or video it&amp;rsquo;s usually better to drop packets (and thus
frames) than to stall a live stream or conversation while syncing up so
streaming applications often use UDP, and it is also commonly used as the
underlying network protocol in FPS games (Quake 3, and probably the other
Quake games spring to mind).&lt;/p&gt;

&lt;p&gt;A major reason for my current love of UDP is because of it&amp;rsquo;s use in StatsD,
a wonderful protocol developed (afaik) at &lt;a href=&#34;https://codeascraft.com/2011/02/15/measure-anything-measure-everything/&#34;&gt;Etsy&lt;/a&gt;
to measure &amp;ldquo;anything and everything&amp;rdquo; in their app.  I came to know StatsD
when I started working for &lt;a href=&#34;http://www.plainvanilla.is&#34;&gt;Plain Vanilla&lt;/a&gt;
who were already using it for monitoring the first single-topic QuizUp
games.  To make a long story short the fire-and-forget approach to massive
amounts of statistical data made my heart stir.&lt;/p&gt;

&lt;h2 id=&#34;a-job-for-udp:45b89691d97c2fd1cd87eabd7efa9032&#34;&gt;A job for UDP!&lt;/h2&gt;

&lt;p&gt;For the last few weeks I&amp;rsquo;ve been working on ways to profile and optimize
&lt;a href=&#34;http://redis.io&#34;&gt;redis&lt;/a&gt; for caching purposes.  The research is
far from conclusive results, but one of the things we needed to do was find
an unobtrusive way to monitor key touches in redis.&lt;/p&gt;

&lt;p&gt;What we decided to do was to patch it to add a &amp;ldquo;key sampling&amp;rdquo; mechanism which
fires UDP packets containing a key when it gets touched.  This allows us to
reather cheaply allow an external program to receive a sampling of key touches
with a minimal performance hit or overhead in redis itself.&lt;/p&gt;

&lt;p&gt;To read this data and pipe it into different analyzers, I hacked up a little
program which listens on an UDP socket and writes anything it receives to
stdout.  Here&amp;rsquo;s a gist of that program:&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;https://gist.github.com/e017ea610a2ee2872e3a.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;I named the gist &amp;ldquo;pointless udp dumper&amp;rdquo;&amp;hellip; so why pointless?&lt;/p&gt;

&lt;p&gt;Well because as a close friend pointed out I could have used &lt;code&gt;netcat&lt;/code&gt;.
My ego was slightly stroked by the fact that he said &amp;ldquo;Ah, I guess you want
to do something more complex later, which is why you don&amp;rsquo;t just use netcat..&amp;ldquo;.&lt;/p&gt;

&lt;p&gt;But the answer was: No, I just keep forgetting that netcat supports UDP,
and these eight bytes (excl. the port!) will do the same as my pointless udp
dumper, on any machine with netcat installed:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;nc -l -u 12345
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;It can be fun to re-invent the wheel, but I must admit that I prefer to do
it consciously :-)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Changing redis maxmemory gradually</title>
      <link>http://steinn.org/post/redis-gradual-maxmemory/</link>
      <pubDate>Sun, 01 Mar 2015 16:28:22 +0000</pubDate>
      
      <guid>http://steinn.org/post/redis-gradual-maxmemory/</guid>
      <description>&lt;p&gt;Today we&amp;rsquo;ve been busy migrating some AWS instances at work due to upcoming
maintenance events in AWS.  One of the instance families we used a lot when
building QuizUp is the m2 instance family and almost all of our m2 instances
will require restarts in the next few days.&lt;/p&gt;

&lt;p&gt;On some of these we are running large redis instances with tens of GB of data
and restarting them is not pain-free, even if we would use the persistence
features of redis, which work by periodically dumping the entire dataset into
a &lt;code&gt;dump.rdb&lt;/code&gt; file.  If the keyspace is tens of GB, the process of saving, and
reading the data is quite slow. Even with a disk subsystem capable of 100MB/s
sustained reads and writes, saving and reading will take around 500 seconds each.&lt;/p&gt;

&lt;p&gt;In the mean time you probably do not want to serve requests.  While saving
because you&amp;rsquo;ll write changes that won&amp;rsquo;t be reflected in the &lt;code&gt;dump.rdb&lt;/code&gt; file, so
you lose data on restart, and while reading the data redis does not allow writes
(at least by default) and even serving reads can be dangerous if your application
makes assumptions based on the availability of keys.&lt;/p&gt;

&lt;p&gt;In order to deal nicely with this scenario we basically slave our redis instances
with newer instance types and then mostly seamlessly failover to them (I can
write another blog post on that later).  Slaving however is not problem-free.&lt;/p&gt;

&lt;p&gt;Until redis 2.8.18 the only way to slave is for the master to start by making a
&lt;code&gt;dump.rdb&lt;/code&gt; file, which when complete gets streamed to the slave, which saves it
to disk, then reads it up from disk, and all writes/changes happening on the
master are buffered in the mean-time.  This causes a series of problems (more
&lt;a href=&#34;http://java.dzone.com/articles/top-redis-headaches-devops&#34;&gt;here&lt;/a&gt;)
which I won&amp;rsquo;t expand on here, but let&amp;rsquo;s suffice to say that the smaller the dataset
is, the easier time you will have making a slave of it.&lt;/p&gt;

&lt;p&gt;If your dataset is mostly volatile, meaning that it&amp;rsquo;s nice to have the data there
but not crucial, lowering the maxmemory down to force redis to evict keys is a
sound strategy to improve your life as a slavemaster.  Today I took a crappy script
I had which does just that and packaged it a little more nicely and it&amp;rsquo;s on GitHub:
&lt;a href=&#34;http://github.com/steinnes/redis-memslider&#34;&gt;steinnes/redis-memslider&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker Workshop</title>
      <link>http://steinn.org/post/docker-workshop/</link>
      <pubDate>Sat, 14 Feb 2015 17:12:57 +0000</pubDate>
      
      <guid>http://steinn.org/post/docker-workshop/</guid>
      <description>

&lt;p&gt;These are the instructions/background I wrote for a workshop on Docker
when we started deploying the backend services for
&lt;a href=&#34;http://www.quizup.com&#34;&gt;QuizUp&lt;/a&gt; as Dockers, in November 2014.&lt;/p&gt;

&lt;h2 id=&#34;what-is-docker:458e3be24730f7f94ff6d0ff1bbbd91d&#34;&gt;What is Docker?&lt;/h2&gt;

&lt;p&gt;It is a server and client for creating lightweight machine images, known
as containers.  Currently these images can be run on Linux, in what are
known as LinuX Containers (LXC).  Docker uses AUFS (a union file system)
and some clever magic to save time when creating new images.&lt;/p&gt;

&lt;h2 id=&#34;how-does-it-do-this:458e3be24730f7f94ff6d0ff1bbbd91d&#34;&gt;How does it do this?&lt;/h2&gt;

&lt;p&gt;On an LXC-capable host, you can run the &amp;ldquo;docker&amp;rdquo; daemon which allows any
docker client to create new images, and start them.  The images are created
using recipe files called &amp;ldquo;Dockerfiles&amp;rdquo;, which look something like this:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt; debian&lt;/span&gt;
&lt;span style=&#34;color: #66d9ef&#34;&gt;ENV&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt; DEBIAN_FRONTEND noninteractive&lt;/span&gt;
&lt;span style=&#34;color: #66d9ef&#34;&gt;RUN&lt;/span&gt; apt-get update &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y openssh-server
&lt;span style=&#34;color: #66d9ef&#34;&gt;ADD&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt; authorized_keys /root/.ssh/authorized_keys&lt;/span&gt;
&lt;span style=&#34;color: #66d9ef&#34;&gt;CMD&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt; mkdir -p /var/run/sshd &amp;amp;&amp;amp; exec /usr/sbin/sshd -D&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;If situated in a folder with a file called &lt;code&gt;Dockerfile&lt;/code&gt; with the contents
from above, we can build an image based on it using the docker command line
client:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;docker build -t steinn/ssh-docker-1 .
Sending build context to Docker daemon  2.56 kB
Sending build context to Docker daemon
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;0&lt;/span&gt; : FROM debian
---&amp;gt; 61f7f4f722fb
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;1&lt;/span&gt; : ENV DEBIAN_FRONTEND noninteractive
---&amp;gt; Running in 18c5aa0fa614
---&amp;gt; f7fb51a8a0ec
Removing intermediate container 18c5aa0fa614
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;2&lt;/span&gt; : RUN apt-get update &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y openssh-server
---&amp;gt; Running in efcaf53c9380
Get:1 http://http.debian.net wheezy Release.gpg &lt;span style=&#34;color: #f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color: #ae81ff&#34;&gt;1655&lt;/span&gt; B&lt;span style=&#34;color: #f92672&#34;&gt;]&lt;/span&gt;
.... VERY LONG PRINTOUT SUPPRESSED ....
---&amp;gt; 6172db1d263d
Removing intermediate container efcaf53c9380
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;3&lt;/span&gt; : CMD mkdir -p /var/run/sshd &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;exec&lt;/span&gt; /usr/sbin/sshd -D
---&amp;gt; Running in 446fb44be23e
---&amp;gt; 914c4e3e928b
Removing intermediate container 446fb44be23e
Successfully built 914c4e3e928b
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Note the &lt;code&gt;---&amp;gt;&lt;/code&gt; commands in between &amp;ldquo;Steps&amp;rdquo;.  They are indicating when
Docker is creating a new file system layer, so in effect an image, and stores
it after every step.  This makes it extremely fast to re-run docker builds
unless the build steps change.  Here is the output from the same command as
above, run again, and with &lt;code&gt;time&lt;/code&gt; in front:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ time &lt;/span&gt;docker build -t steinn/ssh-docker-1 .
Sending build context to Docker daemon  2.56 kB
Sending build context to Docker daemon
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;0&lt;/span&gt; : FROM debian
---&amp;gt; 61f7f4f722fb
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;1&lt;/span&gt; : ENV DEBIAN_FRONTEND noninteractive
---&amp;gt; Using cache
---&amp;gt; f7fb51a8a0ec
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;2&lt;/span&gt; : RUN apt-get update &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y openssh-server
---&amp;gt; Using cache
---&amp;gt; 6172db1d263d
Step &lt;span style=&#34;color: #ae81ff&#34;&gt;3&lt;/span&gt; : CMD mkdir -p /var/run/sshd &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;exec&lt;/span&gt; /usr/sbin/sshd -D
---&amp;gt; Using cache
---&amp;gt; 914c4e3e928b
Successfully built 914c4e3e928b

real        0m0.236s
user        0m0.007s
sys 0m0.008s
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s analyse these messages a bit.  The first one says &amp;ldquo;Sending build
context to Docker daemon  2.56 kB&amp;rdquo;.  What&amp;rsquo;s happening here?  Basically
your docker client is making a tarball of the directory containing your
Dockerfile and all subdirectories, and posting them along with some
metadata to port 2375 of it&amp;rsquo;s Docker host.  The Docker host defaults on
every machine to unix:///var/run/docker.sock, but can be set via the
DOCKER_HOST environment variable to another machine, such as:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ export DOCKER_HOST&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;tcp://192.168.22.8:2375
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;A common approach on OS X machines is to use a set of scripts called
&amp;ldquo;boot2docker&amp;rdquo; which basically give you a very simple interface for
downloading and running a bare-bones linux image with a docker daemon
via virtualbox.  It will even tell you which environment variables to
export.  See:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;$ &lt;/span&gt;boot2docker up
Waiting &lt;span style=&#34;color: #66d9ef&#34;&gt;for&lt;/span&gt; VM and Docker daemon to start...
.o
Started.
Writing /Users/ses/.boot2docker/certs/boot2docker-vm/ca.pem
Writing /Users/ses/.boot2docker/certs/boot2docker-vm/cert.pem
Writing /Users/ses/.boot2docker/certs/boot2docker-vm/key.pem

To connect the Docker client to the Docker daemon, please &lt;span style=&#34;color: #f8f8f2&#34;&gt;set&lt;/span&gt;:
&lt;span style=&#34;color: #f8f8f2&#34;&gt;export DOCKER_HOST&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;tcp://192.168.59.103:2376
&lt;span style=&#34;color: #f8f8f2&#34;&gt;export DOCKER_CERT_PATH&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;/Users/ses/.boot2docker/certs/boot2docker-vm
&lt;span style=&#34;color: #f8f8f2&#34;&gt;export DOCKER_TLS_VERIFY&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;1
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;h1 id=&#34;exercise:458e3be24730f7f94ff6d0ff1bbbd91d&#34;&gt;Exercise&lt;/h1&gt;

&lt;p&gt;Now that you&amp;rsquo;ve played around with docker a bit. It is time for an exercise.&lt;/p&gt;

&lt;p&gt;A few weeks ago we did a little &amp;ldquo;testing kata&amp;rdquo; which was a Python Flask http
web service for making basic calculations.  Since that is a common problem
(ie. creating and dockerizing a little web service) I decided to base todays
exercise on that.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://github.com/plain-vanilla-games/flask-test-kata&#34;&gt;https://github.com/plain-vanilla-games/flask-test-kata&lt;/a&gt;
Clone the repo and checkout the &amp;ldquo;solution&amp;rdquo; branch or use your own older solution&lt;/li&gt;
&lt;li&gt;Create a build folder, within which you write a Dockerfile for this service&lt;/li&gt;
&lt;li&gt;Write a Makefile which builds and tags the docker, and is able to push it to
a Docker registry&lt;/li&gt;
&lt;li&gt;Update the Makefile of your project to run the unit and integration tests
Inside the Docker you just created.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Solution:
&lt;a href=&#34;https://github.com/plain-vanilla-games/flask-test-kata/compare/solution...docker&#34;&gt;https://github.com/plain-vanilla-games/flask-test-kata/compare/solution...docker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When completing this exercise you will likely run into a couple of issues with
the approach of having the &lt;code&gt;Dockerfile&lt;/code&gt; in a separate (&lt;code&gt;build&lt;/code&gt;) folder.  I solve
those issues by using a Makefile &amp;ndash; which in it&amp;rsquo;s most simple form could start
by copying the necessary files from &lt;code&gt;../&lt;/code&gt; and rm-ing them afterwards.&lt;/p&gt;

&lt;p&gt;Since at Plain Vanilla we always tag containers with the githash (commit) of the
code they contain, I decided to hit two birds with one stone, using either the
tip of the current branch or the githash given by the &lt;code&gt;GITHASH&lt;/code&gt; make parameter and
&lt;code&gt;git archive&lt;/code&gt; to deliver the code.&lt;/p&gt;

&lt;p&gt;Additionally, as new timestamps on &lt;code&gt;ADD&lt;/code&gt;-ed files will break the Docker cache I
am extracting the timestamp for the tip of the branch and setting the modification
date for some of the files to that.  Additionally I add requirements.txt manually
so that the &lt;code&gt;RUN pip install -r requirements.txt&lt;/code&gt; step can benefit from the Docker
cache.&lt;/p&gt;

&lt;p&gt;Your first, most basic version of the Makefile could be simpler &amp;ndash; and mine is not
without flaws, but I decided to employ some of the slightly more advanced methods
we use in order to expose some of the trickier parts of using Docker in production.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>brute force still going strong</title>
      <link>http://steinn.org/post/brute-force-still-going-strong/</link>
      <pubDate>Tue, 10 Feb 2015 21:08:21 +0000</pubDate>
      
      <guid>http://steinn.org/post/brute-force-still-going-strong/</guid>
      <description>&lt;p&gt;While setting up this blog I was looking through my little VMs scattered around
different cloud providers to find one which could serve as the A record for
&lt;code&gt;steinn.org&lt;/code&gt; and redirect traffic to &lt;code&gt;steinnes.github.io&lt;/code&gt;.  I logged on to one of my
&lt;a href=&#34;https://www.digitalocean.com&#34;&gt;digital ocean&lt;/a&gt; droplets that I haven&amp;rsquo;t used in
a while so I was slightly surprised to see more than 100k lines in &lt;code&gt;/var/log/auth.log&lt;/code&gt;.&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;https://asciinema.org/a/16270.js&#34; id=&#34;asciicast-16270&#34; async&gt;&lt;/script&gt;

&lt;p&gt;I remember being a teenager and attempting to get access to random systems
I stumbled across on the internet.  I&amp;rsquo;m not going to lie, sometimes I&amp;rsquo;d get in
but the fear of being discovered was more than enough to prevent both any overt
attempts to connect, and to make sure if lucky enough to gain access, no damage
would be done.&lt;/p&gt;

&lt;p&gt;I have the distinct feeling something has changed since the late 90&amp;rsquo;s when I
was pretty much convinced that brute force attacks were just a stupid way to
get caught.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a little &lt;code&gt;iptables&lt;/code&gt; snippet if you&amp;rsquo;re wondering how I made my &lt;code&gt;auth.log&lt;/code&gt;
file stop growing:
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;iptables -F  &lt;span style=&#34;color: #75715e&#34;&gt;# flush&lt;/span&gt;

iptables -A INPUT -p tcp -s your-ip/32 --destination-port &lt;span style=&#34;color: #ae81ff&#34;&gt;22&lt;/span&gt; -j ACCEPT

iptables -A INPUT -p tcp -s 0.0.0.0/0 --destination-port &lt;span style=&#34;color: #ae81ff&#34;&gt;80&lt;/span&gt; -j ACCEPT
iptables -A INPUT -p tcp -s 0.0.0.0/0 --destination-port &lt;span style=&#34;color: #ae81ff&#34;&gt;443&lt;/span&gt; -j ACCEPT

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

iptables -A INPUT -j REJECT  &lt;span style=&#34;color: #75715e&#34;&gt;# reject everything else&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Monolith to Services @ QuizUp</title>
      <link>http://steinn.org/post/utmessan2015/</link>
      <pubDate>Tue, 10 Feb 2015 11:44:14 +0000</pubDate>
      
      <guid>http://steinn.org/post/utmessan2015/</guid>
      <description>

&lt;p&gt;Last week I did a talk at a local IT conference, &lt;a href=&#34;https://www.utmessan.is&#34;&gt;UT Messan&lt;/a&gt;.
The title was the rather inflated &amp;ldquo;&lt;em&gt;From Monolith to Services at Scale: How
QuizUp is making the (inevitable?) transition, one endpoint at a time&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In it I try to tell the story of how we at QuizUp are transitioning to a more
service-oriented architecture, and which steps we decided to take first.&lt;/p&gt;

&lt;p&gt;The route we chose was to use ZooKeeper as the heart of our system for
service registration, discovery and configuration, and then figure out how to
route client requests to services running inside Docker containers.  The
building blocks we ended up using were basically ZooKeeper, NGiNX, Docker, our
own Docker registries (which we refer to as &amp;ldquo;dockistries&amp;rdquo;), as well as a few
custom components which are outlined in the talk:&lt;/p&gt;

&lt;h2 id=&#34;video:72332d114d8f6408c00e9cd6db124848&#34;&gt;video&lt;/h2&gt;

&lt;p&gt;&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;700&#34; height=&#34;400&#34; src=&#34;http://www.youtube.com/embed/GhgH_8-HCVQ&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;

&lt;h2 id=&#34;slides:72332d114d8f6408c00e9cd6db124848&#34;&gt;slides&lt;/h2&gt;

&lt;p&gt;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;a36212c2dcc8418290d98ec6b9c0c8a1&#34; data-ratio=&#34;1.33333333333333&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;

&lt;small&gt;*The slides here above are slightly updated from the ones I used during my talk.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I would have liked to tell a bit more of a story, as we did create a couple
of services in the spring of 2014 which we always had issues deploying. After
some pondering we realized that without some kind of deployable packages (we
rolled our own, and also looked at using .deb), or standardized containers,
and a service registry we would probably end up with a lot of confusion (and
unexpected outages).  We decided on ZooKeeper, Docker and since we&amp;rsquo;re doing
that why not a dynamic router as well.&lt;/p&gt;

&lt;p&gt;Since the timeslot was only 30 minutes so I condensed this into 20-25 minutes
+ questions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>